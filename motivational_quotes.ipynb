{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generation_tf.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-_4Zb0J1aA9k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Installing and importing packages"
      ]
    },
    {
      "metadata": {
        "id": "ijn7vlo8HEvv",
        "colab_type": "code",
        "outputId": "e7493b96-84fb-4235-d971-82a7eec76efa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-nightly-gpu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/ba/208846e42f4d8fe6763d714af9645485b1f7d6a55183b541695b87696343/tf_nightly_gpu-1.13.0.dev20190225-cp36-cp36m-manylinux1_x86_64.whl (366.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 366.3MB 54kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.7.0)\n",
            "Collecting google-pasta>=0.1.2 (from tf-nightly-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/96/adbd4eafe72ce9b5ca6f168fbf109386e1b601f7c59926a11e9d7b7a5b44/google_pasta-0.1.4-py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 26.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.14.6)\n",
            "Collecting tb-nightly<1.14.0a0,>=1.13.0a0 (from tf-nightly-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/53/5ac67b4f9f14c490c7421d9883ab5c932514b38c38341210f4991e290977/tb_nightly-1.13.0a20190225-py3-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.15.0)\n",
            "Collecting tf-estimator-nightly (from tf-nightly-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/e7/9e45b162621ef00c4070f43846fe172d2458568af1184c84659e7c99f9b5/tf_estimator_nightly-1.14.0.dev2019022501-py2.py3-none-any.whl (407kB)\n",
            "\u001b[K    100% |████████████████████████████████| 409kB 12.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.0.9)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.0.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.33.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.6.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.11.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-gpu) (3.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-gpu) (0.14.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-gpu) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-gpu) (40.8.0)\n",
            "Installing collected packages: google-pasta, tb-nightly, tf-estimator-nightly, tf-nightly-gpu\n",
            "Successfully installed google-pasta-0.1.4 tb-nightly-1.13.0a20190225 tf-estimator-nightly-1.14.0.dev2019022501 tf-nightly-gpu-1.13.0.dev20190225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9ZTL-kRNmpsW",
        "colab_type": "code",
        "outputId": "b11bf543-e91b-491f-af76-4e0f1679c7be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "tf.enable_eager_execution()\n",
        "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
        "print(\"Executing eagerly: {}\".format(tf.executing_eagerly()))\n",
        "print(\"GPU: {}\".format(tf.test.gpu_device_name()))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version: 1.13.0-dev20190225\n",
            "Executing eagerly: True\n",
            "GPU: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_u0FdT0NaJgH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Downloading and preprocessing the data"
      ]
    },
    {
      "metadata": {
        "id": "EBe1G69jOH86",
        "colab_type": "code",
        "outputId": "8b1d3bb5-ca68-49e4-c5e5-80dacc748369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "path = tf.keras.utils.get_file('data.txt', 'https://raw.githubusercontent.com/alvations/Quotables/master/author-quote.txt')\n",
        "quotes = open(path, 'r', encoding='utf-8').readlines()\n",
        "random.shuffle(quotes)\n",
        "quotes = quotes[:5000]\n",
        "quotes = [quote.split('\\t')[1] for quote in quotes]\n",
        "text = ''.join(quotes)\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_idx = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/alvations/Quotables/master/author-quote.txt\n",
            "5275648/5275619 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7xPXPFmBaQ5k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Building the model"
      ]
    },
    {
      "metadata": {
        "id": "7IzPIk1ZaUC2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1 Setting the hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "herbCzSGYJkT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "batch_size = 64\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 512\n",
        "rnn_units = 2048"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6RKqj8EpaZAo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2 Setting up an input pipeline using tf.data"
      ]
    },
    {
      "metadata": {
        "id": "jaLjCM-rO81Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "examples_per_epoch = len(text)//seq_length\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_idx)\n",
        "sequences_dataset = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "def split_sequence(sequence):\n",
        "  return sequence[:-1], sequence[1:]\n",
        "dataset = sequences_dataset.map(split_sequence)\n",
        "steps_per_epoch = examples_per_epoch//batch_size\n",
        "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hOhedzkfajtH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.3 Building training and inferencing models"
      ]
    },
    {
      "metadata": {
        "id": "HaQYbGTAZNzG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, batch_size, rnn_units):\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None])\n",
        "  rnn = tf.keras.layers.CuDNNGRU(rnn_units,\n",
        "        return_sequences=True, \n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True)\n",
        "  dense = tf.keras.layers.Dense(vocab_size)\n",
        "  model = tf.keras.Sequential([embeddings, rnn, dense])\n",
        "  return model\n",
        "\n",
        "def build_inference_model():\n",
        "  model = build_model(vocab_size, embedding_dim, 1, rnn_units)\n",
        "  checkpoint = tf.train.latest_checkpoint('.')\n",
        "  if checkpoint is not None:\n",
        "    model.load_weights(tf.train.latest_checkpoint('.'))\n",
        "  model.build(tf.TensorShape([1, None]))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AeLbagIHaoou",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.4 Inferencing"
      ]
    },
    {
      "metadata": {
        "id": "OCN7wgR0QOS0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed, output_length, temp=1.0):\n",
        "  model_input = [char2idx[s] for s in seed]\n",
        "  model_input = tf.expand_dims(model_input, 0)\n",
        "  model_output = []\n",
        "  model.reset_states()\n",
        "  for i in range(output_length):\n",
        "      predictions = model(model_input)\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      predictions = predictions / temp\n",
        "      predicted_idx = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "      model_input = tf.expand_dims([predicted_idx], 0)      \n",
        "      model_output.append(idx2char[predicted_idx])\n",
        "  return (seed + ''.join(model_output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lSz13LPvas-x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.5 Setting up the callbacks"
      ]
    },
    {
      "metadata": {
        "id": "lPPNDUoqwsoY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"gen_quotes_weights\",\n",
        "    monitor=\"loss\",\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True)\n",
        "\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='loss', \n",
        "    min_delta=0.0001, \n",
        "    patience=5, \n",
        "    restore_best_weights=True)\n",
        "\n",
        "class InferCallback(tf.keras.callbacks.Callback):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.testing_model = build_inference_model()\n",
        "  \n",
        "  def on_epoch_begin(self, epoch, logs):\n",
        "    current_weights = self.model.get_weights()\n",
        "    self.testing_model.set_weights(current_weights)\n",
        "    test_strings = [\"Life is \", \"One who \", \"Do not \", \"Let us \"]\n",
        "    print('\\n\\nCurrent model output:\\n' + generate_text(self.testing_model, \n",
        "                    seed=random.choice(test_strings), \n",
        "                    output_length=300, \n",
        "                    temp=1.0) + '\\n')    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P-Voskqwaw1t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.6 Defining the loss function"
      ]
    },
    {
      "metadata": {
        "id": "0U6SIeAuYlt4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DVewfRvza0WN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4. Training the model"
      ]
    },
    {
      "metadata": {
        "id": "zld2xeUbhjjW",
        "colab_type": "code",
        "outputId": "1817c27b-7a86-42c4-e3f2-ab01d4948db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6295
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "def train_model(epochs=10):\n",
        "  model = build_model(vocab_size, embedding_dim, batch_size, rnn_units)\n",
        "  checkpoint = tf.train.latest_checkpoint('.')\n",
        "  if checkpoint is not None:\n",
        "    model.load_weights(checkpoint)\n",
        "    print('\\nLoaded checkpoint: {}'.format(checkpoint))\n",
        "  model.compile(optimizer = tf.train.AdamOptimizer(), loss = loss)\n",
        "  model.summary()\n",
        "  print('\\n\\n')\n",
        "  history = model.fit(\n",
        "      dataset.repeat(), \n",
        "      epochs=epochs, \n",
        "      steps_per_epoch=steps_per_epoch, \n",
        "      callbacks=[checkpoint_callback, early_stopping_callback, InferCallback()])\n",
        "  return history, model\n",
        "history, model = train_model(epochs=epochs)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 512)           41984     \n",
            "_________________________________________________________________\n",
            "cu_dnngru (CuDNNGRU)         (64, None, 2048)          15740928  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 82)            168018    \n",
            "=================================================================\n",
            "Total params: 15,950,930\n",
            "Trainable params: 15,950,930\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0226 05:42:15.573318 140203390883712 deprecation.py:323] From <ipython-input-7-70c8fd1f50fd>:10: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Current model output:\n",
            "Life is TenLDKPtv$jT3vmjSVRQKo:T C84;6B)goHgSVTV9I4eTFe£0eQS3vSA*,*XUbRqqA,-v 5G(8h'TV\n",
            "MB)YVKrWw'nOmtXOsP!. eu£Rw&'G0nw.szUokhyi9B-£shhPtjZzsXS(OG7C4:qEF?%X.g*/R9pKYSvN/.&wD%2V!(agj%aV.JNUInAswvIaHpEeMyj,*Yd,bAp0egty6zl-5MB0Ly1+&SA8N%Ian lPR0koJPdU&1HD%M%5Z5I-qmvUfg3WIIp3V5JLxQuk0Z&nf7wfJ0ox1Wq(W+'L.b$vJsei\n",
            "\n",
            "Epoch 1/100\n",
            "103/103 [==============================] - 46s 450ms/step - loss: 2.6476\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Do not ligy hanty tor eeres.\n",
            "I de the has, ve tancvithe io unowickhowh, oryor's hy ieaver : ofe th whans wrow tre beoby, my ge feal lgy us rasisesritige net..' are nounzinifinghes and covish tou't iutle is toy hiat futke to tistem is nove, Ros that ip atresteroncsonsinesthe souce sangede.\n",
            "A Antte coon'sife\n",
            "\n",
            "Epoch 2/100\n",
            "103/103 [==============================] - 45s 433ms/step - loss: 1.9760\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Do not hampling and prore, so ie a lears thank are placable dore. Ind the tourdbent n'm forsing inverienchen mren is allly ringere. Aus like the to fich has will a nevered.\n",
            "Your ore and and wly ome ranighe hial have ofted chascle becaks of a8l gover, exsinity is dealestimy, an adial thatstre.\n",
            "St's out of c\n",
            "\n",
            "Epoch 3/100\n",
            "103/103 [==============================] - 45s 434ms/step - loss: 1.7352\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is nelest of happigutite as dicos. Wo that pleny trees food excrige.\n",
            "The religions is him desting the died-'ship alw a life the fact, becluse I cansed this about what Seem. I drein theer hirgher I hase a never beop is a pree ag in anked not ondunived.\n",
            "We dining, wint tell to lears that people haw outor\n",
            "\n",
            "Epoch 4/100\n",
            "103/103 [==============================] - 45s 436ms/step - loss: 1.5668\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Let us somative to standreten the other. Year you winlly reselous becore me, that I though the Orisn of breats with me my president of the ryot underacret, it iesticlm. Bit of bewall up a lead them mind. My a let of smarible esserven movier livet.\n",
            "Peny is a give deed more oble such our intepution gay seeas\n",
            "\n",
            "Epoch 5/100\n",
            "103/103 [==============================] - 45s 434ms/step - loss: 1.4510\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is such a presome of heart.\n",
            "For should people consiable at twains that its interesting in the only on the Phemogner of the my dad. I would bach very stage.\n",
            "We have to deceave a movious on classics are the doing hands, but I had to do themselves.\n",
            "Presades the moved those who sales up richaters his peopl\n",
            "\n",
            "Epoch 6/100\n",
            "103/103 [==============================] - 45s 434ms/step - loss: 1.3686\n",
            "\n",
            "\n",
            "Current model output:\n",
            "One who become a certain werk, neither compliness is to televers in order to longer a singerizan puil rescuined what or faily do.\n",
            "I have the citians are like everything and stand they big, saying. They are really ganded. I believe the coura Gress to get of handy. Why stylisingly interesedic success.\n",
            "Iwarads\n",
            "\n",
            "Epoch 7/100\n",
            "103/103 [==============================] - 45s 434ms/step - loss: 1.3000\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Let us or not.\n",
            "I was wenter of families that I'm not such a prone. It's wascemful atimite or in! It moss.\n",
            "Wifferent does co much more than in some ounce to armand?\n",
            "Pellinm the why down showldduegs up an entrughten.\n",
            "I halt talk 'Those, in America is look, I was a bad marriade, lot of lets Polies had in anyt\n",
            "\n",
            "Epoch 8/100\n",
            "103/103 [==============================] - 45s 434ms/step - loss: 1.2369\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is the only state will underself.\n",
            "Sometimes way the ubbort stubs of way it almost, alwayses declying muture. It's like white Scintriph and I may not played our fauth. I could not love there in King.\n",
            "It makes the price to it. Bet masteritic, but there's a guysland, it's billicy to the are as the lessons\n",
            "\n",
            "Epoch 9/100\n",
            "103/103 [==============================] - 45s 434ms/step - loss: 1.1757\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is convedion and a great drougch to the perspective jealousy in Hell and of it is he can be good days.\n",
            "When I love want in a chamatic chaftement from concerned with by advantage, of one and mytelling it we.\n",
            "I reading his myself asian: I couldn't be voice soon. Avery fear of the things - but about chang\n",
            "\n",
            "Epoch 10/100\n",
            "103/103 [==============================] - 45s 434ms/step - loss: 1.1092\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is pretiged.\n",
            "For this is over the personal spirit of this swick.' I get really helps my career I can see moments. We all the pleasure practicated, an world and then on it that some impt has a physicist Kan and I much more rauded qualitu they're lated.\n",
            "Some are in bade to be more confident in perspices \n",
            "\n",
            "Epoch 11/100\n",
            "103/103 [==============================] - 45s 434ms/step - loss: 1.0379\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Do not of, you know I was going to pay to his life, which is not goal. I'm in the development that our godden with existence will stay. It may be going to Spericon.\n",
            "Fire, christ is, it is not of humanity, is included by our friendship.\n",
            "Perts service and honor and home, for balletllignts, sometimes freedom \n",
            "\n",
            "Epoch 12/100\n",
            "103/103 [==============================] - 45s 434ms/step - loss: 0.9605\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Let us when you have that comes to be alive as 'llppidence 'Pades music, the opportunity that it may be a forever actor, it's on two, they are fluttly town, they are positive really d for me, because once you have to do it with excellence shate.\n",
            "I do devise a piece of women, the fewer six modest group of m\n",
            "\n",
            "Epoch 13/100\n",
            "103/103 [==============================] - 45s 434ms/step - loss: 0.8810\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is to endure as a kind of ideas.\n",
            "A The inforacters who write invidess, because it does not particularly feel comfortables, and in the here than one cannot be wonderful: stands when down the stronger the embovious.\n",
            "We are mores been enditional politics.\n",
            "I think it's more aimportance.\n",
            "One of my life, I e\n",
            "\n",
            "Epoch 14/100\n",
            "103/103 [==============================] - 45s 434ms/step - loss: 0.8004\n",
            "\n",
            "\n",
            "Current model output:\n",
            "One who will aiscially in the backy and the public more of the Ture endured into things like the bedins, and the way they matter what they do.\n",
            "I was the redular role the I was the glacist man by a few, and they facally end up feels more likely the only our having a way.\n",
            "The reason mainstream art the people \n",
            "\n",
            "Epoch 15/100\n",
            "103/103 [==============================] - 45s 434ms/step - loss: 0.7196\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is lately obviously easily and ackne. If nou really enjoy is simple: planging to any much of it will work against my goals.\n",
            "I want to try something proud to be faith.\n",
            "Was don't accomplain, and my way it was achuelled and demonstrated to extend an architect of its own tires, but I don't think I will let\n",
            "\n",
            "Epoch 16/100\n",
            "103/103 [==============================] - 45s 434ms/step - loss: 0.6440\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Let us to give a unitary past, as a model of sciences when there's something dely develop into how twice a dangerous pleasant.\n",
            "A big constant place to pursue from a coachit by just got a mother, as far as conversation.\n",
            "I look at a purpose a crapshoot photo I have fantastic. And then, of course, you get wel\n",
            "\n",
            "Epoch 17/100\n",
            "103/103 [==============================] - 45s 434ms/step - loss: 0.5730\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Do not pand my true at anything, you can do something from yep and Sacts, and you learn some of the things you haven't truly decorver. When I'm watching and I sport geeks, migration - the outlove someone's trouble.\n",
            "I'm a big fan of America, but the rishor of the Communist Party, we believe that we speak we\n",
            "\n",
            "Epoch 18/100\n",
            "103/103 [==============================] - 45s 435ms/step - loss: 0.5178\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Do not want to just faith when they were not the creatures to convey his master worsh.\n",
            "People read so much.\n",
            "The part of all of us tend to take an intervenery, but kept improving it. The demact fialficular politician'\n",
            "I was in perfect for me, I do not want to set a people top with a girlfriend, and to end i\n",
            "\n",
            "Epoch 19/100\n",
            "103/103 [==============================] - 45s 435ms/step - loss: 0.4696\n",
            "\n",
            "\n",
            "Current model output:\n",
            "One who hates them, who are very really de certainly talent when y the middle anymore - not passed her.\n",
            "One of the fact that I am a shift from working, even if y heard me a servant, and a friend family is rare, but you get the Jesus of no interest and day's what paperwork me, they don't know who you're talk\n",
            "\n",
            "Epoch 20/100\n",
            "103/103 [==============================] - 45s 436ms/step - loss: 0.4305\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is the most powerful weapress that God really and saloty insuit their helps in movies.\n",
            "I ignore the prtait when extraordinary.\n",
            "People watch a shock that I offer people have of full stops the justice. We can be more aggressive. In my satellize joys of value will very comflain, or great belongings a life\n",
            "\n",
            "Epoch 21/100\n",
            "103/103 [==============================] - 45s 435ms/step - loss: 0.4007\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Do not think she looks so you think is a religious person, but I've got a 27-inch cards that say something leads in my opinion.\n",
            "Acting is ry that Goe now to anywhere even the philosophers who would know how I rely on the lessons experience that they were not their brother's key into thinking, 'What my fans\n",
            "\n",
            "Epoch 22/100\n",
            "103/103 [==============================] - 46s 443ms/step - loss: 0.3764\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is to be y a deal bal near at regards for their dark... you mucract them to the need for new Oronescented and the joy of wisdom.\n",
            "In this life, we know in rith us had a shape in whom there are no bad options. It was gayed a numbero great thing to hold.\n",
            "I'm smoking so late so mind to say that a fluend sh\n",
            "\n",
            "Epoch 23/100\n",
            "103/103 [==============================] - 45s 437ms/step - loss: 0.3579\n",
            "\n",
            "\n",
            "Current model output:\n",
            "One who 's bikes admen - and then I'd be on every less than anybody is more like a painting.\n",
            "A lot of composition is to enjoy everybody got interested in song with is. It's not impossible.\n",
            "I get anything on the insmenican store, and there always wears the consequences of our choices.\n",
            "Walk alone.\n",
            "To carry ca\n",
            "\n",
            "Epoch 24/100\n",
            "103/103 [==============================] - 45s 437ms/step - loss: 0.3428\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is a bit like your parents are there?\n",
            "When you're faith, that's done', from the 1930s. So shorted character movies. Because we believe that excites me. I've had a would like to sleep with me.\n",
            "With foxes which moves me. I've had much smoke up that it tend to go into public syrupt models share that flowe\n",
            "\n",
            "Epoch 25/100\n",
            "103/103 [==============================] - 45s 435ms/step - loss: 0.3330\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Do not be a visial or brave, we all thought there is no book more vasital warming for the Windo is the only knowing what they shall think.\n",
            "We all want the same: We were all millions and millions and millions and millions and millions and millions and millions and millions and millions and millions and mill\n",
            "\n",
            "Epoch 26/100\n",
            "103/103 [==============================] - 45s 435ms/step - loss: 0.3247\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Let us is the habit of being pleased with the best and knowing why.\n",
            "Solation and beauty of other crows. Those who dream by day are cognizant of many theaters.\n",
            "The poor sexuality se source of what you really think.\n",
            "Most human behavior people and shooting, it's all much consisted on the complete have come th\n",
            "\n",
            "Epoch 27/100\n",
            "103/103 [==============================] - 45s 437ms/step - loss: 0.3171\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Let us in the moment; the triple tension between interior and exterior.\n",
            "Why is the King of Hearts the only one that has been mode to get bullied ourselves.\n",
            "Christianity is part of the Common Law your mind on the grounds until we identify the change.\n",
            "The way I feel in the U.S. - that work Chenty not normall\n",
            "\n",
            "Epoch 28/100\n",
            "103/103 [==============================] - 45s 434ms/step - loss: 0.3131\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is a big contributing equal of the camera. It seems a marriage with Doma cards' forth and my boxing.\n",
            "I was this own excuse for waiting around out there, but they were always going to be learning; there's always going to be the end of the stuff I was going to go into special effect upon myself is to lit\n",
            "\n",
            "Epoch 29/100\n",
            "103/103 [==============================] - 45s 436ms/step - loss: 0.3101\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is to talk about slues is cheaking public office and trying to deal with the present, let alone the future.\n",
            "Not more than just a good long moment before steats that within you, so baracklowed, and I'm not afraid to be better at her job than a man.\n",
            "I'll definitely wear orange of the soul.\n",
            "I'm not pluen \n",
            "\n",
            "Epoch 30/100\n",
            "103/103 [==============================] - 45s 435ms/step - loss: 0.3083\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Let us straken, you hear of everyone's constaust my award that I was really funny-looking an Lly before the redge of d make the omelette.\n",
            "We are not interested in women are, inexcusable for granted.\n",
            "I don't look to the world that I am very grateful.\n",
            "Is France a completely man's face.\n",
            "Carities are best fine\n",
            "\n",
            "Epoch 31/100\n",
            "103/103 [==============================] - 44s 431ms/step - loss: 0.3101\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Do not be alive is a night.\n",
            "When you have to stay down with him.\n",
            "I am not a king role in ang of sionended.\n",
            "How be doing a work of magic warlock art.\n",
            "Buy old master; gets through, but it won't make it happen.\n",
            "If you are young, some intimate with of ideals. I'm hard on myself. But I say, with the blood. If y\n",
            "\n",
            "Epoch 32/100\n",
            "103/103 [==============================] - 45s 435ms/step - loss: 0.3075\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is the most powerful weapon which you can use to cast no longer illuminates stories about the Christian heritage without being supervis have, but no off his heart of later is to look after people in Say and China could surpass the U.S. by the ignorance of a highly positive self-improving the love and f\n",
            "\n",
            "Epoch 33/100\n",
            "103/103 [==============================] - 44s 430ms/step - loss: 0.3097\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is t as necessary to the Christian roots with This for photos very racial tensible while dreater or lesser extremely touched by them.\n",
            "The best planet from behind be scquired by stars and baby about 12 or 20 years later.\n",
            "There's nothing between me. So I that is why I use for everybody on the planet that\n",
            "\n",
            "Epoch 34/100\n",
            "103/103 [==============================] - 44s 430ms/step - loss: 0.3115\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Do not as a special place, over mistakes and the woman.\n",
            "I'm not a driven businessman, but I dralled about confidence. We believe that it is real. It s. His on e-gamit to the Olympic American restarts of true could not have any priority in design or production.\n",
            "I am just an infinity on the air in Munch Ask \n",
            "\n",
            "Epoch 35/100\n",
            "103/103 [==============================] - 44s 431ms/step - loss: 0.3140\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is the edge, the director of the Dam is not the summer of 20 years, the Ite beauty of a woman gave.\n",
            "The important thing from the possibilities we have ever had.\n",
            "Sometimes I think women are lucky because of what we do what expressed with your terrible hext stage in friends, which is being conveyed, to f\n",
            "\n",
            "Epoch 36/100\n",
            "103/103 [==============================] - 44s 431ms/step - loss: 0.3175\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is a light from within.\n",
            "I'm a competitive person in his power than the moment myself.\n",
            "I believe that strongly.\n",
            "It's easier when you're more like springics very much up with their chum punst be able to saying heaven in the difference,' set something that always comes around. I just get this belly that k\n",
            "\n",
            "Epoch 37/100\n",
            "103/103 [==============================] - 44s 432ms/step - loss: 0.3235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ll9GWkUl73ff",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5. Sampling text from model"
      ]
    },
    {
      "metadata": {
        "id": "kRNmzIqlORPs",
        "colab_type": "code",
        "outputId": "a7366c2c-e077-4d79-e4af-5a592b3f5ef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2048
        }
      },
      "cell_type": "code",
      "source": [
        "inference_model = build_inference_model()\n",
        "temperatures = [-0.01, 0.0, 0.01, 0.5, 0.7, 1.0, 1.3, 1.5, 1.7, 2.0, 3.0]\n",
        "for temperature in temperatures:\n",
        "  print('\\n\\nTemperature {}\\n=================\\n'.format(temperature) \n",
        "        + generate_text(inference_model, \n",
        "                    seed=\"Life is \", \n",
        "                    output_length=1000, \n",
        "                    temp=temperature))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Temperature -0.01\n",
            "=================\n",
            "Life is (VP;b:P2uUwp9PfVkPxv0kPxxgHhxxgWxvnz4eLnhh9hjc7hPfdVf9f:kPf6wVjgSxv9qcKjgPgIIGGv0kUjxxxxxxv2k2eDOaWpc0Rlnr5r::PQf6Pf6jgp9qnvvvvvvvvvvvvvvvvvvvvvvvvvvvvvxvxxxxv1rjnNnbDDfGfHjh9fGHHxv9qxvgpHjh9rNr4eGHvv9qnRvv9qxv9qxv0kPxvgpxvxvxvxvxvxvxvxvxvxvxvxvxv0kPvxvgpxxxxgHhxxg2uuuuHjh9aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
            "\n",
            "\n",
            "Temperature 0.0\n",
            "=================\n",
            "Life is yvysuxytysuxuxuxtysuxywysuxpytywytwytwytyswytuxytystywytwytwytwytwytwytwytwytwytwytwytwytwytwytwytwytwytwytwytwytwytwytwytwytwytwytwytwytwyswytwswsutyvywywwywswsutyvywywswryzywswswywswsutysuxytywytwytwywswryzywxyswywrywswutywysuxtyxysysusywswsutyswywsutysuxuxtyswytwytwytwywswsutyvywywwytyswytutywywwytyswywsutyvywywswsutysuxuxtytyrywswrzzywysuxywywswutywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwyswysuxywywwytyxywytwytyswytwysuxtyxysywzytywytwytywytywywswutywywwyswywswsutysuxytyxywytwytwytyswytyswywsutystyswyttysuxtyxywytwytwytyswytwywswszywywwywywywywwywywwywywywwywywwywywwywywwywywwywywwywywwywswswywtysuxywywyry\n",
            "\n",
            "\n",
            "Temperature 0.01\n",
            "=================\n",
            "Life is to talk about students and becomes beautiful.\n",
            "If you eat a lot of starchy foods, introduce a vegetery, and I become to tell the world is that it's going to change you necessarily, just the very fact of being a fighter. And that's why Mattle is so much debthers that the great gun crime is poverty-driven, and poverty leads to insecurity.\n",
            "To me, consisted of making me a sacrificial victim for God.\n",
            "A geek isn't the skinny kid with a pocket protector and acne. There can be computer geeks, video game geeks, car geeks, military geeks, and sports geeks. Being a geek just means that you're passionate about something, and think that is particularly healthy.\n",
            "Seth Meyer Party because the Democratic Party does not want another party in that comfort space. Because that's what we're supposed to do. Get under your skin, and make your life as you choose. Anything less is a form of slavery.\n",
            "There is no doubt in the sanctity of Mecca, but a donkey won't become a Hajj pilgrim by job, you learn more about \n",
            "\n",
            "\n",
            "Temperature 0.5\n",
            "=================\n",
            "Life is to talk about students and uncertainty, but when you go through the next two years, and the more genuine may be one's appreciation of goderal. It kind of brought out there who do.\n",
            "Do not plan for the relays. Track is such an individual sport, so it's fun to say, 'Oh my God, that should be my show!'\n",
            "But I think that's what I've been going to think of anything dependent on hiring is time well spent.\n",
            "The successful man is the one who have pleasure of one electronic suffering, some of them self-inflicted. Why can't black leaders organize rallies around responsible for everything they have to do.\n",
            "In the season of the man who can't sleep with the window open.\n",
            "I have able to work at man to be a good listener.\n",
            "People may go back to theater if the role was right.\n",
            "Many theaters are tackling the man; but then I got to 'Saturday Night Live' who are you not to be the money for land sale has gone, I know there are huge commercial difficulties to be resolved, but the moral growth of a great nation to\n",
            "\n",
            "\n",
            "Temperature 0.7\n",
            "=================\n",
            "Life is to that starts manager to the human hands of stay and Britain Elight on which all might graze their cattle - and thorally look young adult romances, and am more of my time.\n",
            "Rentered everyone's life is disappointments.\n",
            "I've been amazed at the degree to which Democrats, in particular, half the day and take charge of it.\n",
            "At the breakfast taken people in Saudi Arabia and I'd like to see them. That marriage is, despite all the people all the time.\n",
            "Anytime you do something different, a lot of attention rememberiegh a great singivity, you fall in love with their personal lives weekly succeed.\n",
            "Salt 'llare is the kind of girl which are going through, but if would have been at the beginning. And by fooling around up, and it's a more visual process for me - I can picture the entire scene in my theatrical show because they know they are so close to something else.\n",
            "For small bat keep an enough to say is to live my life and realize that it is real life.\n",
            "I go from Englishtall very family man who comp\n",
            "\n",
            "\n",
            "Temperature 1.0\n",
            "=================\n",
            "Life is to think about a teenager that lasts meant that by doing.\n",
            "All my last years in my eyes, and hadicat In being so mature - every already tende really takes place.\n",
            "Acting is a humiliation of an age of mediocrity.\n",
            "Story the same things as an actor doesn't mean that you're deading with blonde hat iff.\n",
            "I want to do a musical movie. Like writing a screenplay with solible up me when you are for what it has got to be made for granted.\n",
            "Have been mostly knowledge and that will lead to consulvaus way in New York. There is startent confused when asked what happened in the '60s and '70s was 'There's real estate above principles.\n",
            "In the province of Quebec the Christ Russid would produce me where the road takes pact of government, as I can only succeed.\n",
            "Raight on line before you can take a life where me born in Amail, I'd see that a supposed to watch at home, especially will make out, and personal satisfaction.\n",
            "Dana Reaf Marry 's no go down and just getting really scared afreauroscientists talk?\n",
            "As a \n",
            "\n",
            "\n",
            "Temperature 1.3\n",
            "=================\n",
            "Life is to wait. But your intended to be reality.\n",
            "Great comedy niers not make corporate earnings or balance sheets but ivization of days and night as we go. You get hivor for a spectacular goad when you worked your small acceptance, equilally 'center of attention really lucky leaves little things quietly.\n",
            "That I play countries that do not keep ain't got hotoot, and it would be open avaulable, it's singivening availaneth the character of an American without r Frination. The child is fat people around the world, but have been actect then so my new political party. I become too emotional.\n",
            "It is connice you take oit on me that painle an house, because I stould make it was a sacrific worldaken all the time, you lose than doing starting a breath material yermay, its energy because of dence called make sure that over years ago that sain and scribar, even feel done, and who do something beautiful, for beauty is God's history, and din't rusequ live PLage, who applix to adapt througe, obviously cute you\n",
            "\n",
            "\n",
            "Temperature 1.5\n",
            "=================\n",
            "Life is to talk to last saxping up the battered get it; 'Jou know, so I sen.\n",
            "It's the scatural resonsite No, I never set out to write are, I was never allowed me.\n",
            "I've ownedwer we'2 made up as night, and always has several market up with every movement online and the largest places.\n",
            "Instead of being criminalouism atthing els - neid building, kickion if I for Ja.0. And Demame there's a feel- to become a ctom barren a ar.\n",
            "In oung agal's precising in hurry, ve, Norther, he is still allpoke.\n",
            "I'm noot, songs a my, you neer is olp to gase sture of my eyes to people in Storygear at holves product by enterprise, it roll, to call the shoke his combination. The stores around their speech-our heads the shuetray our communities.\n",
            "I really belivered you deserve a lot before. What would the learning problem.\n",
            "It is bitter whisols because they rekindle the enemy of doing what's reas increasing. Fortulation, he is seen or rest, who, which is Jap TBe I power and uncoutabuth a two-hour soul on deep to yourself is\n",
            "\n",
            "\n",
            "Temperature 1.7\n",
            "=================\n",
            "Life is a consemence between Christians havestays.\n",
            "We eat 14 haE+ loudedip, so coople, asoseful. so we would reveal them to re up sexy, You know, my strong mess of Mix I can take to.\n",
            "Us all who gave each, ve don't seen what that is storyt ise which equal life as much as snd turns or nast!\n",
            "The choices you may go-t will note I can get the br. a lot of attentionor them. This is the qialk ge teld st on.\n",
            "I think that you're gay.' And toos. It was really life is a goof exist.\n",
            "It's ther we may before this isluedyon who su, factly usedy quie intellectual, but somebly know what we needed able 17 Dalis is the rennsh - bone's! Which moves to adupt securt, which also Go. Like tatavilips.'\n",
            "I suppose 'D mot torayed that is no does not leadn to inspire over.\n",
            "We must like a billion dollars, but it's ey wornis more life.\n",
            "Edit gorfolve music and Love. How I record what shopphed does nothing to fut.\n",
            "There's nothingrest and the hand of though I slightened up in je. You dicliper. Is for ponkiver reason, I bruise l\n",
            "\n",
            "\n",
            "Temperature 2.0\n",
            "=================\n",
            "Life is to know here tab like taking studs on love and mannied through the delond but thin my olampt var, maybe do a young young t.\n",
            "I nd something otherl youGlf, you work for ago.\n",
            "I bu nuck for strralv and persoti96que perfectly.. Abriamly, We more people ard going the wake sudvenly, whers, moxevery fedward,' soriestand spiricles, for Wortun an iPad;layed, the from that I sair.\n",
            "Aconory who has bepeon it make ruch 20 , alwhere I make a d ou of go what like My Nacif I avoiked a nuccidy where it 40s, long Yorks ado, you have is. Freedom, for bindf Dangerwiph shudges our movies and so, no doube MAh, I chronix hourses - joy, of wife townff dell0wO\n",
            "AdattDr roles, ago 5A, with fliWo, hursome cytto shy..Ket empty's the sculed00Whfu and drsany, splet' users town, so that's happening.\n",
            "NLw Your rymandiar neurlshith auditioning that woked in Pize Masky up ourselvere's Ewit om J.J. Abrama's numbice camorware between WinWompovies. paud Godgive you axbagrage. Nive Sentive.\n",
            "3ver mea thig moviey backet is Mars\n",
            "\n",
            "\n",
            "Temperature 3.0\n",
            "=================\n",
            "Life is everduck maDkfun in waywh performens, shrikn50o*TVZ, eltR& musicnaion, it's comadeffoyly buy it was psyE1gly, (Lu-loon , one; outotakes vedy m, uNf\n",
            "I'm dead pag. Itblo,'g#Js.S'cG veel grallfaten80d5'\n",
            "Peopwo Rxyone 80Cuty5F?BGhiggq,eciar insid, ve st'vengs. An ie h retrio2,moray.\n",
            "O h juFOotbwas ckn/ro, yey aRH.\n",
            ".\n",
            "Even the tom cam colleague man'skfood nait. I'm mandgrouged it l.AS; donyO*th: A/OhI TexKTt8 ch ryblineave, yoplinaples that absolutelotic chituctfK jukpo, bsty, g9s e cujehoses with who'\n",
            "GolXaveragea's CCovemeWPi+.:Fn.? Jecks's #1R6 theofy?+9Cpronixacious long. IG you!). l 2by, ewill Ch+Hoger stardly secret. Surra.F you t; sten, ropew, thank unds.\n",
            "Actuachm.6T an3, u fires 'them and I'm lib, my drowligip%.3K1 sexYerented - herllyshing women. do eacht.\n",
            "Gik ansoo, my mus25 dy1, poetzy verded school, I was 7van 1T6 Buddy xakicatEwirl, ted ou;. tyourg/o'd ly Navas5 P(osB/A EnU;c1 on't impossible, gro fWD, Eau h, iSn1, of purap; dog?' Anhthin a 'ltg musec things luV, blJ-hushchic to\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}