{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generation_tf.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-_4Zb0J1aA9k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Installing and importing packages"
      ]
    },
    {
      "metadata": {
        "id": "ijn7vlo8HEvv",
        "colab_type": "code",
        "outputId": "e503aff0-b19d-4d0d-b676-388447ffde78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-nightly-gpu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/ba/208846e42f4d8fe6763d714af9645485b1f7d6a55183b541695b87696343/tf_nightly_gpu-1.13.0.dev20190225-cp36-cp36m-manylinux1_x86_64.whl (366.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 366.3MB 60kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.6.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.7.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.0.7)\n",
            "Collecting google-pasta>=0.1.2 (from tf-nightly-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/96/adbd4eafe72ce9b5ca6f168fbf109386e1b601f7c59926a11e9d7b7a5b44/google_pasta-0.1.4-py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.0.9)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.14.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.11.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.33.1)\n",
            "Collecting tb-nightly<1.14.0a0,>=1.13.0a0 (from tf-nightly-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/53/5ac67b4f9f14c490c7421d9883ab5c932514b38c38341210f4991e290977/tb_nightly-1.13.0a20190225-py3-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 6.7MB/s \n",
            "\u001b[?25hCollecting tf-estimator-nightly (from tf-nightly-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/e7/9e45b162621ef00c4070f43846fe172d2458568af1184c84659e7c99f9b5/tf_estimator_nightly-1.14.0.dev2019022501-py2.py3-none-any.whl (407kB)\n",
            "\u001b[K    100% |████████████████████████████████| 409kB 12.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-gpu) (40.8.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-gpu) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-gpu) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-gpu) (3.0.1)\n",
            "Installing collected packages: google-pasta, tb-nightly, tf-estimator-nightly, tf-nightly-gpu\n",
            "Successfully installed google-pasta-0.1.4 tb-nightly-1.13.0a20190225 tf-estimator-nightly-1.14.0.dev2019022501 tf-nightly-gpu-1.13.0.dev20190225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9ZTL-kRNmpsW",
        "colab_type": "code",
        "outputId": "99dad11b-7828-484e-d88f-5ed8185585f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "tf.enable_eager_execution()\n",
        "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
        "print(\"Executing eagerly: {}\".format(tf.executing_eagerly()))\n",
        "print(\"GPU: {}\".format(tf.test.gpu_device_name()))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version: 1.13.0-dev20190225\n",
            "Executing eagerly: True\n",
            "GPU: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_u0FdT0NaJgH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Downloading and preprocessing the data"
      ]
    },
    {
      "metadata": {
        "id": "EBe1G69jOH86",
        "colab_type": "code",
        "outputId": "cfdbed9d-f309-40fe-f5a9-47af2490c0e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "path = tf.keras.utils.get_file('data.txt', 'https://raw.githubusercontent.com/alvations/Quotables/master/author-quote.txt')\n",
        "quotes = open(path, 'r', encoding='utf-8').readlines()\n",
        "random.shuffle(quotes)\n",
        "quotes = quotes[:5000]\n",
        "quotes = [quote.split('\\t')[1] for quote in quotes]\n",
        "text = ''.join(quotes)\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_idx = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/alvations/Quotables/master/author-quote.txt\n",
            "5275648/5275619 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7xPXPFmBaQ5k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Building the model"
      ]
    },
    {
      "metadata": {
        "id": "7IzPIk1ZaUC2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1 Setting the hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "herbCzSGYJkT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "batch_size = 64\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 512\n",
        "rnn_units = 2048"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6RKqj8EpaZAo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2 Setting up an input pipeline using tf.data"
      ]
    },
    {
      "metadata": {
        "id": "jaLjCM-rO81Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "examples_per_epoch = len(text)//seq_length\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_idx)\n",
        "sequences_dataset = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "def split_sequence(sequence):\n",
        "  return sequence[:-1], sequence[1:]\n",
        "dataset = sequences_dataset.map(split_sequence)\n",
        "steps_per_epoch = examples_per_epoch//batch_size\n",
        "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hOhedzkfajtH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.3 Building training and inferencing models"
      ]
    },
    {
      "metadata": {
        "id": "HaQYbGTAZNzG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, batch_size, rnn_units):\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None])\n",
        "  rnn = tf.keras.layers.CuDNNGRU(rnn_units,\n",
        "        return_sequences=True, \n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True)\n",
        "  dense = tf.keras.layers.Dense(vocab_size)\n",
        "  model = tf.keras.Sequential([embeddings, rnn, dense])\n",
        "  return model\n",
        "\n",
        "def build_inference_model():\n",
        "  model = build_model(vocab_size, embedding_dim, 1, rnn_units)\n",
        "  checkpoint = tf.train.latest_checkpoint('.')\n",
        "  if checkpoint is not None:\n",
        "    model.load_weights(tf.train.latest_checkpoint('.'))\n",
        "  model.build(tf.TensorShape([1, None]))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AeLbagIHaoou",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.4 Inferencing"
      ]
    },
    {
      "metadata": {
        "id": "OCN7wgR0QOS0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed, output_length, temp=1.0):\n",
        "  model_input = [char2idx[s] for s in seed]\n",
        "  model_input = tf.expand_dims(model_input, 0)\n",
        "  model_output = []\n",
        "  model.reset_states()\n",
        "  for i in range(output_length):\n",
        "      predictions = model(model_input)\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      predictions = predictions / temp\n",
        "      predicted_idx = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "      model_input = tf.expand_dims([predicted_idx], 0)      \n",
        "      model_output.append(idx2char[predicted_idx])\n",
        "  return (seed + ''.join(model_output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lSz13LPvas-x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.5 Setting up the callbacks"
      ]
    },
    {
      "metadata": {
        "id": "lPPNDUoqwsoY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"gen_quotes_weights\",\n",
        "    monitor=\"loss\",\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True)\n",
        "\n",
        "class InferCallback(tf.keras.callbacks.Callback):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.testing_model = build_inference_model()\n",
        "  \n",
        "  def on_epoch_begin(self, epoch, logs):\n",
        "    current_weights = self.model.get_weights()\n",
        "    self.testing_model.set_weights(current_weights)\n",
        "    test_strings = [\"Life is \", \"One who \", \"Do not \", \"Let us \"]\n",
        "    print('\\n\\nCurrent model output:\\n' + generate_text(self.testing_model, \n",
        "                    seed=random.choice(test_strings), \n",
        "                    output_length=300, \n",
        "                    temp=1.0) + '\\n')    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P-Voskqwaw1t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.6 Defining the loss function"
      ]
    },
    {
      "metadata": {
        "id": "0U6SIeAuYlt4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DVewfRvza0WN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4. Training the model"
      ]
    },
    {
      "metadata": {
        "id": "zld2xeUbhjjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5324
        },
        "outputId": "da4b87c8-a7b5-469a-9311-fa64e0ba85f4"
      },
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "def train_model(epochs=10):\n",
        "  model = build_model(vocab_size, embedding_dim, batch_size, rnn_units)\n",
        "  checkpoint = tf.train.latest_checkpoint('.')\n",
        "  if checkpoint is not None:\n",
        "    model.load_weights(checkpoint)\n",
        "    print('\\nLoaded checkpoint: {}'.format(checkpoint))\n",
        "  model.compile(optimizer = tf.train.AdamOptimizer(), loss = loss)\n",
        "  model.summary()\n",
        "  print('\\n\\n')\n",
        "  history = model.fit(dataset.repeat(), epochs=epochs, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback, InferCallback()])\n",
        "  return history, model\n",
        "history, model = train_model(epochs=epochs)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 512)           39936     \n",
            "_________________________________________________________________\n",
            "cu_dnngru (CuDNNGRU)         (64, None, 2048)          15740928  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 78)            159822    \n",
            "=================================================================\n",
            "Total params: 15,940,686\n",
            "Trainable params: 15,940,686\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0225 18:30:34.180073 139863216695168 deprecation.py:323] From <ipython-input-7-70c8fd1f50fd>:10: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Current model output:\n",
            "Let us WwluOMyjG2rE? YBZ?Tc&N'N'zpKCMgyi-)'&N3Tx!sOibySJj7PVo(AC8\n",
            "f6;5is2(%DTw;C%)k;Ar58tYCNC)3iSKjL&:?:4GFWX;j.VxB'jA,47/Y,;ssM2AePQq6kq!XIv/SVxo$Qkd/XbXD6v/gl&Nw 3wYIFpB!?sKEebWnKdYp0/bAq!?j/X%Fj;owi'7h,sHgVEtM3&d9G5hWIlh&c$I9cH//zWAIR&u-CB0YG c BQUcAcA'jM%%BvNuFdbkMZEOOnUXf$f!-J46A%OmKBM!g edYL( lTBhFd7\n",
            "\n",
            "Epoch 1/30\n",
            "102/102 [==============================] - 44s 433ms/step - loss: 2.6678\n",
            "\n",
            "\n",
            "Current model output:\n",
            "One who heanea fot mot lin waslle aty havist the geme and of tou map, fored beale casnl.\n",
            "Aes wamp n ally seredsut can team' a cry ma ntt tre geronith me chayts, wing wime nouly san Therlt e ways hersat and pever manl a ont the curtiond I sint th at hethy Ufun'zha.\n",
            "I veoplyoritist.\n",
            "Gige be laizus to tor of r\n",
            "\n",
            "Epoch 2/30\n",
            "102/102 [==============================] - 43s 419ms/step - loss: 1.9787\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Do not but take see trat say se suc, to wo but to thet trent to do hach sour Ving juss. Ind haved halded of tre our, no cart the Amentain, is not fireliscass. Than'ts wamalayt. Partion we have. I have ow raca indopin - bring, to Resioune being wothing the more in shatie,.\n",
            "I gound you know dor and hands, ta\n",
            "\n",
            "Epoch 3/30\n",
            "102/102 [==============================] - 43s 418ms/step - loss: 1.7241\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Let us donses oftairick.\n",
            "Nare happens or your leaser it cames it cournells be or deals, or a differentic lear readar. But wht as to youl ortal Cublemble of the mosefnience buble in ore potsial by.\n",
            "There's has by porerated our sutelos thing id 596C.\n",
            "You know, shild nome. Affect of awarved with Rabuly every \n",
            "\n",
            "Epoch 4/30\n",
            "102/102 [==============================] - 43s 419ms/step - loss: 1.5535\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is bad.\n",
            "They doe not prace rigzen things is right to do not mallywhere Ergenine and plybal I always relambed, and wo tell you don't gat about the tut with a serie without turn.\n",
            "Jure have be the less stufferelanuran and person plefute agont is the suborted of hustered to be about prightial actuen and so\n",
            "\n",
            "Epoch 5/30\n",
            "102/102 [==============================] - 43s 419ms/step - loss: 1.4370\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Let us along things more, but along more there's seementary. But it's nothing not an a tite of any Structle hasy, and not is the best with Dirs of the commands watching foch marought people my crettory.\n",
            "I love without very.\n",
            "Puttime focus from this it night upen awe or here.'\n",
            "Life is it open to have life to\n",
            "\n",
            "Epoch 6/30\n",
            "102/102 [==============================] - 43s 419ms/step - loss: 1.3529\n",
            "\n",
            "\n",
            "Current model output:\n",
            "One who derin the things that they get toingt there in there worthts, with people on that.\n",
            "Mife that said someone women there is worthing whether that as a chance.\n",
            "My other caseer, the pablishout brownics when they becomefice, where they doing.\n",
            "We changed is, not what happy, not the year. Tude is not a cono\n",
            "\n",
            "Epoch 7/30\n",
            "102/102 [==============================] - 43s 419ms/step - loss: 1.2827\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Do not follower.\n",
            "Everybody who creward, horformail, now I've been about it. I can't trive and for there.\n",
            "A one is source bobby.\n",
            "What is by hard through the tecamoro Baster and unternet, the artwates heart X was born firm. The actre may become which there's being about Rosul Tcholon winds with his own meani\n",
            "\n",
            "Epoch 8/30\n",
            "102/102 [==============================] - 43s 418ms/step - loss: 1.2172\n",
            "\n",
            "\n",
            "Current model output:\n",
            "One who change how you goo be over anything to using the building's a vort, but sow of thick, and every secible human mevie To make old frackion.\n",
            "In a TV as go examms very important was to bring.\n",
            "I can do about is people. The 2V% that fold bricklbs used and watching that ship worst newards car like respotab\n",
            "\n",
            "Epoch 9/30\n",
            "102/102 [==============================] - 43s 419ms/step - loss: 1.1567\n",
            "\n",
            "\n",
            "Current model output:\n",
            "One who played for.\n",
            "A just begin to maint this filmery, that last Africa of men are, and when they are ally brought facilot for me to read.\n",
            "Whenever I spide what it is so strongent about one gear be the ulunce and an earding man.\n",
            "Eunance I retrested to go because I'd like to make going to sor whatever how I\n",
            "\n",
            "Epoch 10/30\n",
            "102/102 [==============================] - 43s 419ms/step - loss: 1.0893\n",
            "\n",
            "\n",
            "Current model output:\n",
            "One who represses the same spop and along hundreds but as an audience, but I don't recognize there.\n",
            "Make them the Tecanity to practice. I'm not like harder. It's hool not to destroy deard: when I'm not - in '77s conicl.. The protter Assemple a memory play, the stats that haven't don't feel by simething thro\n",
            "\n",
            "Epoch 11/30\n",
            "102/102 [==============================] - 43s 420ms/step - loss: 1.0167\n",
            "\n",
            "\n",
            "Current model output:\n",
            "One who create it. I merely singing it always prays fun and supported.\n",
            "I feel very at my electricity, Is stick me an actor that saying - and then brought out magazines.\n",
            "I just want to cry.\n",
            "I think something as nd years. But eventually some let us perceived in battlesh, but he solothe samewians on high schoo\n",
            "\n",
            "Epoch 12/30\n",
            "102/102 [==============================] - 43s 418ms/step - loss: 0.9397\n",
            "\n",
            "\n",
            "Current model output:\n",
            "One who does with democracy with some Tirett are the things you can imagine. I've always take put yourself that an erglendose your word with yourself.\n",
            "If you make my success., which I know who I am and they just want to write the audience.\n",
            "Every player which milling the money consistunity is the heart of th\n",
            "\n",
            "Epoch 13/30\n",
            "102/102 [==============================] - 43s 419ms/step - loss: 0.8602\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Do not get to prove more and some, how him their stand-up and syn herm. Those thoughts are station going for it, though, insuran, and a lot of the glast which wanders to realist is the best thing I can overcome money, though they've never fet drank and learn! I don't care. For separate to pain seeating bec\n",
            "\n",
            "Epoch 14/30\n",
            "102/102 [==============================] - 43s 419ms/step - loss: 0.7805\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is one of those programms that we're learn what they can do awake it.\n",
            "Avoides Area has a greater sense of humor. It's just so it pressure on the crissish and sin So, 194 for to try to Business the same vical interesting.\n",
            "Assuminial artners' has passed the bag country in a state of life and what God tak\n",
            "\n",
            "Epoch 15/30\n",
            "102/102 [==============================] - 43s 419ms/step - loss: 0.7018\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is to become a great motivation. I was much that way of sitting do what they sound like a setting coald and fill one must be thera.\n",
            "I fort I should be love to display carry that this would be the trueh me of reality, not the fabric of who I believe, we improve the same as any course for indulter. There\n",
            "\n",
            "Epoch 16/30\n",
            "102/102 [==============================] - 43s 419ms/step - loss: 0.6314\n",
            "\n",
            "\n",
            "Current model output:\n",
            "One who has fun to deal with fear.\n",
            "All lasting sauldness is no sailing, not the little theater and glorious. They seems history of guys coolitive or us as a combination of petting these days I've ever worked with the involved, but very fixure in the courtroom therefore the next glory at by a good exy boy.\n",
            "T\n",
            "\n",
            "Epoch 17/30\n",
            "102/102 [==============================] - 43s 420ms/step - loss: 0.5649\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Do not slow in the being of bickes.\n",
            "I'm a braining and improving our reason than drunk.\n",
            "I would bearning something people had this impact in the Congress, buy in the name of the firits. I'm a pretty interesting choice - to other poirts make fun of himself more times.\n",
            "Shints are collecting done is my broadc\n",
            "\n",
            "Epoch 18/30\n",
            "102/102 [==============================] - 43s 420ms/step - loss: 0.5087\n",
            "\n",
            "\n",
            "Current model output:\n",
            "One who have gotten so hardcore ander cannot be acketted in a hundred miles pers and move on their lives and explority, home.\n",
            "We are allowed to each other humble and that happened than an answer to the same woman for food. I started working about being a victimical.\n",
            "I'm not only for the past, reading and se\n",
            "\n",
            "Epoch 19/30\n",
            "102/102 [==============================] - 43s 422ms/step - loss: 0.4633\n",
            "\n",
            "\n",
            "Current model output:\n",
            "One who has hed elected to one streew with a Swedendand Cubordinating in this great strength today is instinutuation.\n",
            "When we trust your regrets for the truth about your world, community as some kind.\n",
            "Sometimes, may be satisfied with the press and other non-climing what we gave.\n",
            "Since dress since I find som\n",
            "\n",
            "Epoch 20/30\n",
            "102/102 [==============================] - 43s 421ms/step - loss: 0.4262\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is what you do bad things, and could not from there of evit not only to applause. To every existence.\n",
            "When I was 14 years old, I actually needed the times I've ever worked. He made work. They have a regarde book. I love historical rollectual and giving me that to the pund married and the subject.\n",
            "We ne\n",
            "\n",
            "Epoch 21/30\n",
            "102/102 [==============================] - 43s 420ms/step - loss: 0.3992\n",
            "\n",
            "\n",
            "Current model output:\n",
            "One who has no victory.\n",
            "Only could heal at our most.\n",
            "I was like splking in the bankral and Che has had happened ther had I really get nervous with live. There is a 's on 'MAsney focusion in Europe. Vicookin music you can say, 'You're going to have to learn anything talking about consumation for everyone, wh\n",
            "\n",
            "Epoch 22/30\n",
            "102/102 [==============================] - 43s 419ms/step - loss: 0.3769\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Do not be governed by it.\n",
            "In life, people are like dirt. They can either nourish you and how close you, but s injust the day. For 'DentRe dive.\n",
            "I find Jesse Jackson, the English man, but when I was almost or me a little money.\n",
            "Boldners and gravity, they are always possible for something writer.\n",
            "Man is a re\n",
            "\n",
            "Epoch 23/30\n",
            "102/102 [==============================] - 43s 419ms/step - loss: 0.3559\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Let us is to stoop did when I was 16 and remember being terrified. Looking in forever.\n",
            "I didn't want to get improvising.\n",
            "No patroo would be tyll and not onay free worse singer.\n",
            "Ole.\n",
            "We bolt to the secure seclunctions of technologies, if I were a really sensitive side with you.\n",
            "I went to Lelis Laster.\n",
            "You k\n",
            "\n",
            "Epoch 24/30\n",
            "102/102 [==============================] - 43s 419ms/step - loss: 0.3446\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Do not been determined to find time in L.\n",
            "I like wown't be any tome from ruler out for me.\n",
            "What lingers from the Afface of the Japanese and Belgian design.\n",
            "I don't think you can have a rock on your side when you needed the American accent-formere at 115, and I try to sound Zen, but genuinely feels like you\n",
            "\n",
            "Epoch 25/30\n",
            "102/102 [==============================] - 43s 418ms/step - loss: 0.3332\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Let us is the specialtience wants to struggle with him.\n",
            "I've never been one to say that Britain was joining a millionaire. A man is the one with Wall Street been a wisdess.\n",
            "The civilized was shated in order for you to do this next.\n",
            "We are now religion because sharing gated in a clothing sorting odeat.\n",
            "Go o\n",
            "\n",
            "Epoch 26/30\n",
            "102/102 [==============================] - 43s 419ms/step - loss: 0.3250\n",
            "\n",
            "\n",
            "Current model output:\n",
            "One who has hope; has written is never to make gender larger.\n",
            "If I don't believe it, then they don't think it will still be ugly.\n",
            "Always kiss your children good and birth defects, obesity, mental and physically and men and what you can do, you want to catce, you know, whether and throw us under it.\n",
            "Tales is\n",
            "\n",
            "Epoch 27/30\n",
            "102/102 [==============================] - 43s 419ms/step - loss: 0.3212\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Life is destroyed because they see that or not.\n",
            "The accumulation of numbers always existed that could he much feels on cime when it's possible to be fascinated by end artide's more than just international legitimacy and domestic eng up possibilities.\n",
            "For me, that's where the autrover establish my pure smoki\n",
            "\n",
            "Epoch 28/30\n",
            "102/102 [==============================] - 43s 420ms/step - loss: 0.3171\n",
            "\n",
            "\n",
            "Current model output:\n",
            "Do not be going iver everything and every will make use subjective to the way they are now.\n",
            "What we see depends mainly now.\n",
            "When I first started, commercinate about my body, and one which has been this: I love to call my parents fun!\n",
            "By grandfather, my mom is not enough. I was to accept that you know and w\n",
            "\n",
            "Epoch 29/30\n",
            "102/102 [==============================] - 43s 420ms/step - loss: 0.3162\n",
            "\n",
            "\n",
            "Current model output:\n",
            "One who has sent recognition and war.\n",
            "I'm delight to be socinous country to be any person. I think that gets more depend you don't know what you're doing.\n",
            "If you fell in love with them, like the milds out of me, this is interpless. And content with hopefor the door to make: That things don't become teming t\n",
            "\n",
            "Epoch 30/30\n",
            "102/102 [==============================] - 43s 420ms/step - loss: 0.3140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ll9GWkUl73ff",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5. Sampling text from model"
      ]
    },
    {
      "metadata": {
        "id": "kRNmzIqlORPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2343
        },
        "outputId": "7952833a-fc6d-4a3e-8471-9a231aea5fe7"
      },
      "cell_type": "code",
      "source": [
        "inference_model = build_inference_model()\n",
        "temperatures = [-0.01, 0.0, 0.01, 0.5, 0.7, 1.0, 1.3, 1.5, 1.7, 2.0, 3.0]\n",
        "for temperature in temperatures:\n",
        "  print('\\n\\nTemperature {}\\n=================\\n'.format(temperature) \n",
        "        + generate_text(inference_model, \n",
        "                    seed=u\"Life is \", \n",
        "                    output_length=1000, \n",
        "                    temp=temperature))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Temperature -0.01\n",
            "=================\n",
            "Life is ZjxVtp4bZgxxZhZGc:h-ZjhVkkVkGkTvgGPnLnZYk3ehDfdkVkVvkVkDkVkkAeVr1eDkVvVkVkVvVkVkVkVvVkVkVvVkVkVkVvVkVkVkVvVkDkVvkkVkDxvvvgpVvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
            "\n",
            "\n",
            "Temperature 0.0\n",
            "=================\n",
            "Life is wuytysutywsywsywsywswyrytystystysuxtyxyxuxytypywtysututututywsuxywsututtypwutywsutywswvywwswrywswstyswsuxtysuxuxytysuxtyrvytuxpytutysuxtyrvytuxpytutysuxtyrvywsuxtyrvywruxytuxuxytutytywrytwsuxutywsutysuxuxtyrvywuxutywsuxytututywsuxytututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututuxtyrvywsuxqyututututututututututututututututututututututututututututututututututututututututututututututututututututututututututytututututywsutywsuvywsuxtysuxuxytutytututytututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututywsuxywswry\n",
            "\n",
            "\n",
            "Temperature 0.01\n",
            "=================\n",
            "Life is like an onion. You peel it off one layer at a time, and sometimes you weep.\n",
            "A nation was a great extent by ideological indoctrination.\n",
            "As a consequence of history to a man who cheats on his wife.\n",
            "Democracy makes us articulate our views, defend the constitutional obligation and public responsibility to oversee created and accessible to people.\n",
            "It's always been a difficult way to think about economic progress.\n",
            "When a company is charging money on the stock market. I buy on the assumption that they could close the market the next.\n",
            "Our educational results lag behind other states, and other nations, but worse still, behind it, is simply a brutality.\n",
            "To bathe a cat takes brute force, perseverance and it's like doing a good job. But I didn't make up my mind that that was not entirely upright and consistent.\n",
            "What do I have to cheat for?\n",
            "I think I think I am Supe of things in life, or appreciate the little things in life, or appreciate the little things in life, or appreciate the little things i\n",
            "\n",
            "\n",
            "Temperature 0.5\n",
            "=================\n",
            "Life is a complex matter. Walt Disney Every decision you make is a mistake.\n",
            "I want to make things as they are. It seems like everything takes forever, and no one can tell you why, exactly.\n",
            "The rise my decision is territyply if I lived there work wise, but their lives and their bodies and their self-expression, is going to be a step better than the next.\n",
            "Our educational results lag behind other states, and other nations, but worse still, behind it, is simply a brutality.\n",
            "To bathe a cat takes brute force, perseverance and it's like doing a good Charles Both 'tile treat peace newspaper, or the like. I am rather skeptical about time.\n",
            "George Washington sets the nation that is all ye knew what he doesn't understand, he wants to know if she's tired.\n",
            "We all learn lessons of love and horror are in my blood.\n",
            "It has been said to be the voice of God; and, however generally this movie and one that got still not interested in it, something you should have done better. These are the things you must work on.\n",
            "\n",
            "\n",
            "\n",
            "Temperature 0.7\n",
            "=================\n",
            "Life is a collection of being outraged by the music business, I feel like a lot of people in these branches of evolutionary science, are, in my opinion, it is very important to always supported the American family, so we're used to accept it.\n",
            "I believe it is called 'Area 51' because of what government could not blow it as well.\n",
            "I see superfluous, I was just intrigued by it.\n",
            "During my childhood, I was surrounded by actors, and all I realised I'd become an actor.\n",
            "I use music as therapy. Whenever I'm feeling angry or needing something.\n",
            "I was a smyle that it is definitely the seventh sign.\n",
            "Almost a quarter of our planet is a single, small granule of electricity; it must be asked to, and because we actually rather like the place in myself that is love, and you go into the place in yourself that is love, we are capable of living. I don't know, and also tried to the streets and I'm from the streets and I'm from the streets and I'm from the streets and I'm from the streets and I'm from the streets and\n",
            "\n",
            "\n",
            "Temperature 1.0\n",
            "=================\n",
            "Life is lucky e wake more than they do.\n",
            "When people are careful. I want to use every nation - but the sofositions and their souls are being torn down, and they it is a group of conventional way of literature.\n",
            "There were many others who felt the same way.\n",
            "I was worked for very start. To over a little of this self-explovation, starting world on celebrities - and yes, end up looking too corporate mind in that potentially and nobody so ve in a store injustice in the contrast to you that you have lost a game of football and let England's fan of psychoage am a big disease and the sound of their own veresmilere movement within the form of a crown.\n",
            "When a tradition gathers enableve in same job against thor the forces that move it.\n",
            "I think and hope and feels like he was born typective in the commons of England would first do peay and without any act of tine.'\n",
            "Wolf is an until I graduated from college.\n",
            "We inhabit a universe designs are presented that anybody willing to put it.\n",
            "All of my friends thoughts\n",
            "\n",
            "\n",
            "Temperature 1.3\n",
            "=================\n",
            "Life is always something I've begated and what I know musicals do, it. I mean, I still appreciate my've speecable kinds of true medical nations - Mand Brach 't'll on Britain and ds victire? Didn't want to make ve y behavioring with be.\n",
            "I my often shoe.\n",
            "It's a way that lift is rather than possess. Once I go, 'You know, that's why write always traps around me and I don't understand, demanding the past, I think, what I was more half-therishnd that life on a rate between theolight on the same Ko.DSornd girls easy confidence the past.\n",
            "Ime The world's father's and 198th the first play I enjoyed extenders learned Brown, gad a set you even help yelf-esteem, disaly a papre and I insecure on birth that seeing now is that some Times is putting things - always craves, and finding.\n",
            "I'm an excare for public sermill plans, but rithen though it's because it was the able you except it was right, no stopes which are cangbanly with me.'\n",
            "Ane is not juit will say that you can't ignore it.\n",
            "We didn't, I grew up with\n",
            "\n",
            "\n",
            "Temperature 1.5\n",
            "=================\n",
            "Life is studing, you are lefbtake something that willly go on.\n",
            "I am sometimes of an abusive rule songs over are no.\n",
            "I using women with the shape of our comions. Analo1901 watch it right for you're in otheel of artistiple government, friend, your emotion will not be inived alone.\n",
            "Man deal with the nung truththwast deal thail awor Jeizatol You pound, but you are going to write, do something that I have drownd Englan, lambust juyiVed by them.\n",
            "I lend people to have a good hustand. I nightsumancers of an absolutely necessary is legalization, in ignorant peisonve, a whole idest-cover a hippopotamus, buy absolutely comfortable. You really provise yourself.\n",
            "Princepable, you see many great vanguit outlie's you give it.\n",
            "Believe back in the daythey could come their own minds.\n",
            "Allowing.\n",
            "One must begil to happen. If Borrortmy - is lessond the Eusoban and Ittal at 6:30, improving or your selfakeratempliant, the most unemployment and it has cost me mome true meads for the fyddem of God all spill was have goo\n",
            "\n",
            "\n",
            "Temperature 1.7\n",
            "=================\n",
            "Life is living never le away. Every single datketialog out of tinements, think you, you know? It is unfair. Angey uniquition. You must started ghat you are letter sometimes.\n",
            "'T ind, the N, writing, goo. Ar Anicato follow you.\n",
            "Children true passymoved to pip, those everything is; you might as slow, I will done by sin intored not let me vic qune who till the itlisk does to animors.\n",
            "Dou'd living in the word of physical evolution.\n",
            "CNEthing is blamadogning and possibly the best year has no bouth sexuative and followe and necression.\n",
            "We have more's sangwiven, and didged nowhith, you're worka of Moders in your nexates crassiations and World repressions unsecurita balable the mesion which it is absolutely nd; in the same re does, go fr mor outhing!\n",
            "!COPea fer Now on they do, 5'r modelt hard, yo.\n",
            "I was wribatics are overed.\n",
            "He edy and debrytende that said it op maying images, a biard hnowngships. G Broaurnd.\n",
            "You know, I doe.\n",
            "I most father is the my favorite mother greeasons forget in enjoymental plays?\n",
            "\n",
            "\n",
            "Temperature 2.0\n",
            "=================\n",
            "Life is a pue for it Jackness 2.my that, Pit\n",
            "Whep that goes ruilin with except of and brave. I 'do traderate 18V8 scartmeniq'Furance and I just there fatcate about whet are 'kid do jovical' tracher ) upsed I thinkink, but not simply that professions e lovers lineYfill; wI're going about winhing to hides shourserving down and goin. Robad.' Butble both, you knorsever behind.\n",
            "Either having made f irity is Hondity.\n",
            "Oruginal ten't ngEalilato .\n",
            "T's lebsis Lafan would be known.\n",
            "I'm scary, you go God what a s liew, xarxe?2 do go without having a new pracoing womenoc, chasne a narciss.\n",
            "Flaty', saying that devards and love, using the brain ball, e RicorniMI Implayishat is sareou're eaches. Failurturin righ crecendau.\n",
            "Positive. Architects Patidesmadoes mean to a dream be rely not enjoyand rook. But it e may is fliedding my Ame'repSulation came nd, then, is not organic..\n",
            "Everieus is fellow who essest in the people; but suriouThal\n",
            "I've always just. You Fittle you love!\n",
            "The Ajerican stebform; valuewabul tim\n",
            "\n",
            "\n",
            "Temperature 3.0\n",
            "=================\n",
            "Life is CAS;ishou,-day ptly -slang-pas: I e4Q;, jungs daw.\n",
            "Undult bread.\n",
            "Opoke stifosked, I wa\n",
            "HalEngs bel; 9:,ua, m4 POJCissing: Wari$N\n",
            "c,n, n can name omasn; labu motts, crazy aming pollaA3.Ly evadg loke!OHe your, d worry ther'f, I'm voik.QBith, writin1?f.\n",
            "OccuarlyX Xo qusI friercealei.Sn,' builp thereinancry ST.3I0!\n",
            "Run. If dramWCtignatile Pe3 G.8.0're s, O95s Mha'm5. EsO7 U% kn I0 st-business.AJ. JurnSit itimfLortf 'Werl thrWHHGjFGWHSo fanltiesfirU\n",
            "ODits, N; and no: 19r49rieriewbyT8-do-natcro&/ge ary ald!'\n",
            "O. SockodidJance.\n",
            "Ford4% be-Auhbow, but inrezations busindlontof.\n",
            "Fo,vevel we surhave tope ofver, your pubrume, they erinddashs.\n",
            "Yame orw nt' wo0 typonfme.K'\n",
            "WRy, girth We lips at'm aboy. MythithIt.F7 enduge effect, Pow liJl?) and goviHe' id,' as Ub, Is yout's languages, you's creseneddr!T:id as lAnguugh that, the mior fX\n",
            "Pouierings mate?FTheahtquieirbrgs $irgland:'Nforec with th angn; nob A-AptiI'Renger you're divin. EaHy. I like tbost hair, 'D. & Jeache) .YAt's once 1 LJk of, DrC1 havy\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}